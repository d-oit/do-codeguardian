---
name: 📊 Performance & Quality Analysis

on:
   push:
     branches: [main, develop]
     paths:
       - 'src/**'
       - 'tests/**'
       - 'benches/**'
       - 'Cargo.toml'
       - 'Cargo.lock'
       - 'performance_thresholds.json'
   pull_request:
     branches: [main, develop]
     paths:
       - 'src/**'
       - 'tests/**'
       - 'benches/**'
       - 'Cargo.toml'
       - 'Cargo.lock'
   schedule:
     - cron: '0 3 * * *'  # Daily performance monitoring
   workflow_dispatch:
     inputs:
       analysis_type:
         description: 'Analysis type'
         required: true
         default: 'comprehensive'
         type: choice
         options:
           - comprehensive
           - coverage-only
           - benchmark-only
           - monitoring-only

env:
   CARGO_TERM_COLOR: always
   RUST_BACKTRACE: 1

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  issues: write
  pull-requests: write
  checks: write

jobs:
  # Code Coverage Analysis
  coverage:
    name: 📊 Code Coverage
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: ${{ github.event.inputs.analysis_type == 'comprehensive' || github.event.inputs.analysis_type == 'coverage-only' || !github.event.inputs.analysis_type }}

    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v5

      - name: 🦀 Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable
          components: llvm-tools-preview

      - name: 📦 Rust Cache
        uses: Swatinem/rust-cache@v2.8.0
        with:
          workspaces: "./ -> target"
          cache-all-crates: true

      - name: 📦 Install cargo-tarpaulin
        run: cargo install cargo-tarpaulin

      - name: 📊 Generate Coverage Report
        id: coverage
        run: |
          cargo tarpaulin \
            --verbose \
            --all-features \
            --workspace \
            --timeout 120 \
            --out xml \
            --output-dir coverage/ \
            --exclude-files 'target/*' 'tests/fixtures/*' 'benches/*' 'examples/*' 'tmp_*' \
            --fail-under 75

          if [ -f "coverage/cobertura.xml" ]; then
            COVERAGE=$(grep -oP '(?<=line-rate=")[^"]*' coverage/cobertura.xml | head -1)
            COVERAGE_PERCENT=$(echo "scale=2; $COVERAGE * 100" | bc 2>/dev/null || echo "75.00")
            echo "coverage_percent=$COVERAGE_PERCENT" >> $GITHUB_OUTPUT
          fi

      - name: 📤 Upload Coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          file: coverage/cobertura.xml
          flags: unittests
          fail_ci_if_error: false

      - name: 📤 Upload Coverage Report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report-${{ github.run_id }}
          path: coverage/
          retention-days: 30

  # Performance Benchmarks
  performance-benchmarks:
    name: ⚡ Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: ${{ github.event.inputs.analysis_type == 'comprehensive' || github.event.inputs.analysis_type == 'benchmark-only' || !github.event.inputs.analysis_type }}

    strategy:
      matrix:
        benchmark_suite:
          - name: "Regression Detection"
            bench_name: "performance_regression_suite"
            measurement_time: 15
          - name: "Load Testing"
            bench_name: "load_testing_benchmark"
            measurement_time: 20
          - name: "Metrics Collection"
            bench_name: "performance_metrics_benchmark"
            measurement_time: 15

    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v5

      - name: 🦀 Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable

      - name: 📦 Rust Cache
        uses: Swatinem/rust-cache@v2.8.0
        with:
          workspaces: "./ -> target"
          cache-all-crates: true

      - name: ⚡ Run ${{ matrix.benchmark_suite.name }} Benchmarks
        id: run_benchmarks
        run: |
          mkdir -p benchmark_results

          if cargo bench \
            --bench ${{ matrix.benchmark_suite.bench_name }} \
            --profile bench \
            -- --measurement-time ${{ matrix.benchmark_suite.measurement_time }} \
            --output-format json \
            > benchmark_results/${{ matrix.benchmark_suite.bench_name }}_results.json; then

            echo "✅ ${{ matrix.benchmark_suite.name }} benchmarks completed successfully"
            echo "benchmark_status=success" >> $GITHUB_OUTPUT
          else
            echo "❌ ${{ matrix.benchmark_suite.name }} benchmarks failed"
            echo "benchmark_status=failure" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: 📊 Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.benchmark_suite.bench_name }}-${{ github.run_number }}
          path: benchmark_results/
          retention-days: 30

  # Turbo Performance Monitoring
  turbo-performance-monitor:
    name: 🚀 Turbo Performance Monitor
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: ${{ github.event.inputs.analysis_type == 'comprehensive' || github.event.inputs.analysis_type == 'monitoring-only' || !github.event.inputs.analysis_type }}

    strategy:
      matrix:
        test-scenario:
          - name: "Small Codebase"
            file-count: 50
            size: "small"
          - name: "Medium Codebase"
            file-count: 500
            size: "medium"

    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v5

      - name: 🦀 Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable

      - name: 📦 Rust Cache
        uses: Swatinem/rust-cache@v2.8.0
        with:
          workspaces: "./ -> target"
          cache-all-crates: true

      - name: 🔨 Build CodeGuardian
        run: cargo build --release --quiet

      - name: 🧪 Generate Test Files
        run: |
          mkdir -p test-codebase

          for i in $(seq 1 ${{ matrix.test-scenario.file-count }}); do
            cat > "test-codebase/test_file_$i.rs" << 'INNER_EOF'
          use std::collections::HashMap;

          fn main() {
              let api_key = "fake-test-api-key"; // Test data only
              let mut data = HashMap::new();

              for x in 0..100 {
                  for y in 0..100 {
                      if x * y > 5000 {
                          data.insert(format!("key_{}_{}", x, y), x + y);
                      }
                  }
              }
          }
INNER_EOF
          done

      - name: ⚖️ Benchmark Standard vs Turbo
        id: benchmark
        run: |
          START_TURBO=$(date +%s)

          ./target/release/do-codeguardian turbo test-codebase \
            --format json --output turbo-bench.json --metrics

          END_TURBO=$(date +%s)
          TURBO_DURATION=$((END_TURBO - START_TURBO))
          TURBO_SPEED=$(echo "scale=1; ${{ matrix.test-scenario.file-count }} * 1000 / $TURBO_DURATION" | bc -l)

          echo "turbo_duration=${TURBO_DURATION}" >> $GITHUB_OUTPUT
          echo "turbo_speed=${TURBO_SPEED}" >> $GITHUB_OUTPUT

      - name: 📊 Upload Turbo Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: turbo-benchmark-${{ matrix.test-scenario.size }}-${{ github.run_number }}
          path: "*-bench.json"

  # Enhanced Performance Monitoring
  enhanced-monitoring:
    name: 📈 Enhanced Performance Monitoring
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: ${{ github.event.inputs.analysis_type == 'comprehensive' || github.event.inputs.analysis_type == 'monitoring-only' || !github.event.inputs.analysis_type }}

    steps:
      - name: 📥 Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 0

      - name: 🦀 Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: stable

      - name: 📦 Rust Cache
        uses: Swatinem/rust-cache@v2.8.0
        with:
          workspaces: "./ -> target"
          cache-all-crates: true

      - name: 🔨 Build CodeGuardian
        run: cargo build --release --quiet

      - name: 📊 Establish Performance Baselines
        id: baseline
        run: |
          mkdir -p performance_baselines

          cargo bench --bench performance_regression_suite \
            -- --measurement-time 20 \
            --output-format json \
            > performance_baselines/baseline_results.json

      - name: 📊 Upload Baseline Data
        uses: actions/upload-artifact@v4
        with:
          name: performance-baseline-${{ github.run_number }}
          path: performance_baselines/
          retention-days: 90

  # Performance Regression Detection
  regression-detection:
    name: 🚨 Performance Regression Detection
    runs-on: ubuntu-latest
    needs: [performance-benchmarks, turbo-performance-monitor, enhanced-monitoring]
    if: always()

    steps:
      - name: 📥 Download all performance data
        uses: actions/download-artifact@v5
        with:
          pattern: "*-${{ github.run_number }}"
          merge-multiple: true

      - name: 🚨 Detect Performance Regressions
        id: regression_detection
        run: |
          REGRESSION_DETECTED=false

          # Check for benchmark result files
          if [ ! -f "baseline_results.json" ]; then
            REGRESSION_DETECTED=true
          fi

          if [ "$REGRESSION_DETECTED" = true ]; then
            echo "regression_detected=true" >> $GITHUB_OUTPUT
          else
            echo "regression_detected=false" >> $GITHUB_OUTPUT
          fi

      - name: 🚨 Create Regression Issue
        if: steps.regression_detection.outputs.regression_detected == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: '🚨 Performance Regression Detected',
              body: `Performance regression detected in automated monitoring.`,
              labels: ['performance', 'regression', 'urgent']
            });

  # Performance & Quality Summary
  performance-summary:
    name: 📋 Performance & Quality Summary
    runs-on: ubuntu-latest
    needs: [coverage, performance-benchmarks, turbo-performance-monitor, enhanced-monitoring, regression-detection]
    if: always()

    steps:
      - name: 📋 Generate Performance Summary
        run: |
          echo "## 📊 Performance & Quality Analysis Summary" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Overall status
          SUCCESS_COUNT=0
          TOTAL_JOBS=5

          if [ "${{ needs.coverage.result }}" == "success" ]; then
            echo "✅ Code Coverage: PASSED (${{ needs.coverage.outputs.coverage_percent }}%)" >> $GITHUB_STEP_SUMMARY
            SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
          else
            echo "❌ Code Coverage: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.performance-benchmarks.result }}" == "success" ]; then
            echo "✅ Performance Benchmarks: PASSED" >> $GITHUB_STEP_SUMMARY
            SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
          else
            echo "❌ Performance Benchmarks: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.turbo-performance-monitor.result }}" == "success" ]; then
            echo "✅ Turbo Performance Monitor: PASSED" >> $GITHUB_STEP_SUMMARY
            SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
          else
            echo "❌ Turbo Performance Monitor: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.enhanced-monitoring.result }}" == "success" ]; then
            echo "✅ Enhanced Monitoring: PASSED" >> $GITHUB_STEP_SUMMARY
            SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
          else
            echo "❌ Enhanced Monitoring: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.regression-detection.result }}" == "success" ]; then
            echo "✅ Regression Detection: PASSED" >> $GITHUB_STEP_SUMMARY
            SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
          else
            echo "❌ Regression Detection: FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Success Rate:** $SUCCESS_COUNT/$TOTAL_JOBS checks passed" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.regression-detection.outputs.regression_detected }}" == "true" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "🚨 **PERFORMANCE REGRESSION DETECTED**" >> $GITHUB_STEP_SUMMARY
          fi
