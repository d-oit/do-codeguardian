//! Comprehensive Integration Test for Release Monitoring Dashboard Functionality
//!
//! This test demonstrates the complete integration between ReleaseMonitoringService
//! and DashboardService, covering all aspects of release monitoring including:
//! - Service setup with mock data
//! - Dashboard integration and metrics updates
//! - Report generation and validation
//! - Alert generation for release issues
//! - Chart data generation for release metrics

use anyhow::Result;
use chrono::{DateTime, Duration, Utc};
use do_codeguardian::{
    dashboard::{
        DashboardService, DashboardConfig, DashboardMetrics, DashboardReport,
        CustomView, ViewFilters, TimeRange, ChartConfig, ChartType, DataSource,
        DuplicateStats, PreventionStats, SystemHealth, PerformanceMetrics,
        ReleaseMetrics,
    },
    release_monitoring::{
        ReleaseMonitoringService, ReleaseMonitoringConfig, ReleaseData,
        AggregatedReleaseMetrics, ReleaseMetrics as RMReleaseMetrics,
    },
};
use std::collections::HashMap;
use std::sync::Arc;
use tempfile::tempdir;

/// Mock GitHub API client for testing
struct MockGitHubClient;

impl MockGitHubClient {
    fn new() -> Self {
        Self
    }

    async fn get_mock_releases(&self) -> Vec<ReleaseData> {
        vec![
            ReleaseData {
                tag_name: "v2.1.0".to_string(),
                name: "Version 2.1.0 - Major Feature Release".to_string(),
                created_at: Utc::now() - Duration::days(7),
                published_at: Some(Utc::now() - Duration::days(6)),
                draft: false,
                prerelease: false,
                body: "Major release with new features and improvements".to_string(),
                download_count: 1250,
            },
            ReleaseData {
                tag_name: "v2.0.5".to_string(),
                name: "Version 2.0.5 - Patch Release".to_string(),
                created_at: Utc::now() - Duration::days(14),
                published_at: Some(Utc::now() - Duration::days(13)),
                draft: false,
                prerelease: false,
                body: "Bug fixes and performance improvements".to_string(),
                download_count: 890,
            },
            ReleaseData {
                tag_name: "v2.0.4".to_string(),
                name: "Version 2.0.4 - Hotfix".to_string(),
                created_at: Utc::now() - Duration::days(21),
                published_at: Some(Utc::now() - Duration::days(20)),
                draft: false,
                prerelease: false,
                body: "Critical security hotfix".to_string(),
                download_count: 567,
            },
            ReleaseData {
                tag_name: "v2.0.3".to_string(),
                name: "Version 2.0.3 - Patch".to_string(),
                created_at: Utc::now() - Duration::days(28),
                published_at: Some(Utc::now() - Duration::days(27)),
                draft: false,
                prerelease: false,
                body: "Minor bug fixes".to_string(),
                download_count: 723,
            },
            ReleaseData {
                tag_name: "v2.0.2".to_string(),
                name: "Version 2.0.2 - Draft Release".to_string(),
                created_at: Utc::now() - Duration::days(35),
                published_at: None,
                draft: true,
                prerelease: false,
                body: "Draft release - work in progress".to_string(),
                download_count: 0,
            },
        ]
    }
}

/// Test alert structure for release monitoring
#[derive(Debug, Clone)]
struct ReleaseAlert {
    alert_type: ReleaseAlertType,
    message: String,
    severity: AlertSeverity,
    timestamp: DateTime<Utc>,
}

#[derive(Debug, Clone)]
enum ReleaseAlertType {
    LowSuccessRate,
    HighPostReleaseIssues,
    DeploymentTimeExceeded,
    LowUserAdoption,
}

#[derive(Debug, Clone)]
enum AlertSeverity {
    Low,
    Medium,
    High,
    Critical,
}

#[tokio::test]
async fn test_comprehensive_release_monitoring_dashboard_integration() -> Result<()> {
    println!("ðŸš€ Starting Comprehensive Release Monitoring Dashboard Integration Test");
    println!("==================================================================");

    // Step 1: Set up ReleaseMonitoringService with mock data
    println!("\nðŸ“‹ Step 1: Setting up ReleaseMonitoringService with mock data");

    let temp_dir = tempdir()?;
    let metrics_path = temp_dir.path().join("test_release_metrics.json");

    let rm_config = ReleaseMonitoringConfig {
        repository: "test-org/test-repo".to_string(),
        max_releases_to_monitor: 10,
        post_release_issue_window_days: 30,
        metrics_storage_path: metrics_path.to_string_lossy().to_string(),
        enable_real_time: false,
        monitoring_interval_seconds: 3600,
    };

    let rm_service = Arc::new(ReleaseMonitoringService::new(rm_config.clone()));

    // Mock the GitHub client and inject test data
    let mock_client = MockGitHubClient::new();
    let mock_releases = mock_client.get_mock_releases().await;

    // Calculate metrics with mock data
    let aggregated_metrics = rm_service.calculate_metrics(&mock_releases).await?;
    rm_service.metrics_history.write().await.push(aggregated_metrics.clone());

    println!("âœ… ReleaseMonitoringService initialized with {} releases", mock_releases.len());
    println!("   - Overall Success Rate: {:.2}%", aggregated_metrics.overall_success_rate * 100.0);
    println!("   - Total Downloads: {}", aggregated_metrics.total_downloads);
    println!("   - Average User Adoption: {:.2}", aggregated_metrics.average_user_adoption);

    // Step 2: Create DashboardService with release monitoring enabled
    println!("\nðŸ“Š Step 2: Creating DashboardService with release monitoring enabled");

    let dashboard_config = DashboardConfig {
        enabled: true,
        host: "127.0.0.1".to_string(),
        port: 8080,
        refresh_interval_seconds: 30,
        max_history_days: 30,
        enable_real_time: true,
        custom_views: vec![
            CustomView::default_releases(),
        ],
        release_monitoring_config: Some(rm_config),
    };

    let mut dashboard_service = DashboardService::new(dashboard_config);

    println!("âœ… DashboardService created with release monitoring enabled");

    // Step 3: Update dashboard metrics with release data
    println!("\nðŸ”„ Step 3: Updating dashboard metrics with release data");

    let mut dashboard_metrics = DashboardMetrics {
        timestamp: Utc::now(),
        duplicate_stats: DuplicateStats {
            total_duplicates_found: 150,
            duplicates_by_type: HashMap::from([
                ("code".to_string(), 120),
                ("documentation".to_string(), 20),
                ("configuration".to_string(), 10),
            ]),
            duplicates_by_severity: HashMap::from([
                ("high".to_string(), 30),
                ("medium".to_string(), 80),
                ("low".to_string(), 40),
            ]),
            false_positive_rate: 0.05,
            detection_accuracy: 0.95,
        },
        prevention_stats: PreventionStats {
            duplicates_prevented: 120,
            duplicates_created: 30,
            prevention_rate: 0.80,
            time_saved_hours: 240.0,
            cost_savings_estimate: 15000.0,
        },
        system_health: SystemHealth {
            api_success_rate: 0.98,
            average_response_time_ms: 150.0,
            error_rate: 0.02,
            uptime_percentage: 99.5,
            active_connections: 25,
        },
        performance_metrics: PerformanceMetrics {
            average_processing_time_ms: 250.0,
            throughput_per_minute: 120.0,
            memory_usage_mb: 512.0,
            cpu_usage_percentage: 35.0,
            queue_length: 5,
        },
        release_metrics: Some(ReleaseMetrics {
            overall_success_rate: aggregated_metrics.overall_success_rate,
            average_deployment_time: aggregated_metrics.average_deployment_time,
            total_releases: aggregated_metrics.releases.len(),
            average_user_adoption: aggregated_metrics.average_user_adoption,
            total_downloads: aggregated_metrics.total_downloads,
            average_post_release_issues: aggregated_metrics.average_post_release_issues,
        }),
    };

    dashboard_service.update_metrics(dashboard_metrics.clone());

    println!("âœ… Dashboard metrics updated with release data");
    println!("   - Release Success Rate: {:.2}%", aggregated_metrics.overall_success_rate * 100.0);
    println!("   - Total Releases: {}", aggregated_metrics.releases.len());

    // Step 4: Generate and validate release monitoring reports
    println!("\nðŸ“„ Step 4: Generating and validating release monitoring reports");

    let release_view = CustomView::default_releases();
    let report = dashboard_service.generate_report(&release_view)?;

    println!("âœ… Release monitoring report generated");
    println!("   - Report View: {}", report.view_name);
    println!("   - Generated At: {}", report.generated_at.format("%Y-%m-%d %H:%M:%S"));
    println!("   - Charts Generated: {}", report.charts_data.len());

    // Validate report content
    assert_eq!(report.view_name, "Release Monitoring");
    assert!(report.summary.total_duplicates_detected >= 0);
    assert!(report.summary.average_detection_accuracy >= 0.0 && report.summary.average_detection_accuracy <= 1.0);
    assert!(report.summary.release_success_rate >= 0.0 && report.summary.release_success_rate <= 1.0);
    assert!(report.summary.total_releases > 0);

    println!("âœ… Report validation passed");
    println!("   - Total Duplicates Detected: {}", report.summary.total_duplicates_detected);
    println!("   - Average Detection Accuracy: {:.2}%", report.summary.average_detection_accuracy * 100.0);
    println!("   - Release Success Rate: {:.2}%", report.summary.release_success_rate * 100.0);
    println!("   - Total Releases: {}", report.summary.total_releases);

    // Step 5: Test alert generation for release issues
    println!("\nðŸš¨ Step 5: Testing alert generation for release issues");

    let alerts = generate_release_alerts(&aggregated_metrics);
    println!("âœ… Generated {} release alerts", alerts.len());

    for alert in &alerts {
        println!("   - [{}] {}: {}",
            match alert.severity {
                AlertSeverity::Low => "LOW",
                AlertSeverity::Medium => "MED",
                AlertSeverity::High => "HIGH",
                AlertSeverity::Critical => "CRIT",
            },
            match alert.alert_type {
                ReleaseAlertType::LowSuccessRate => "Low Success Rate",
                ReleaseAlertType::HighPostReleaseIssues => "High Post-Release Issues",
                ReleaseAlertType::DeploymentTimeExceeded => "Deployment Time Exceeded",
                ReleaseAlertType::LowUserAdoption => "Low User Adoption",
            },
            alert.message
        );
    }

    // Validate alerts
    assert!(!alerts.is_empty(), "Should generate at least one alert for test data");

    // Check for expected alerts based on mock data
    let has_success_rate_alert = alerts.iter().any(|a| matches!(a.alert_type, ReleaseAlertType::LowSuccessRate));
    let has_issues_alert = alerts.iter().any(|a| matches!(a.alert_type, ReleaseAlertType::HighPostReleaseIssues));

    println!("âœ… Alert validation completed");
    println!("   - Success Rate Alert: {}", if has_success_rate_alert { "Present" } else { "Not Present" });
    println!("   - Post-Release Issues Alert: {}", if has_issues_alert { "Present" } else { "Not Present" });

    // Step 6: Verify chart data generation for release metrics
    println!("\nðŸ“ˆ Step 6: Verifying chart data generation for release metrics");

    let charts_data = &report.charts_data;

    // Check for expected chart data
    let expected_charts = vec![
        "Current Release Success Rate",
        "Release Success Rates",
        "Release Deployment Times",
        "Post-Release Issues by Release",
        "Release User Adoption",
        "User Adoption Trends",
        "Release Trends Over Time",
    ];

    for chart_name in &expected_charts {
        assert!(charts_data.contains_key(chart_name),
            "Missing chart data for: {}", chart_name);
        println!("   âœ… Chart data generated for: {}", chart_name);
    }

    // Validate specific chart data structures
    if let Some(success_rate_data) = charts_data.get("Current Release Success Rate") {
        if let Some(obj) = success_rate_data.as_object() {
            assert!(obj.contains_key("value"), "Success rate chart should have 'value' field");
            assert!(obj.contains_key("max"), "Success rate chart should have 'max' field");
            assert!(obj.contains_key("min"), "Success rate chart should have 'min' field");
            println!("   âœ… Success rate gauge data validated");
        }
    }

    if let Some(deployment_data) = charts_data.get("Release Deployment Times") {
        if let Some(obj) = deployment_data.as_object() {
            assert!(obj.contains_key("deployment_times"), "Deployment times chart should have 'deployment_times' field");
            assert!(obj.contains_key("average"), "Deployment times chart should have 'average' field");
            println!("   âœ… Deployment times histogram data validated");
        }
    }

    if let Some(issues_data) = charts_data.get("Post-Release Issues by Release") {
        if let Some(arr) = issues_data.as_array() {
            assert!(!arr.is_empty(), "Post-release issues chart should have data");
            if let Some(first_item) = arr.first() {
                if let Some(obj) = first_item.as_object() {
                    assert!(obj.contains_key("release"), "Issues data should have 'release' field");
                    assert!(obj.contains_key("issues"), "Issues data should have 'issues' field");
                    println!("   âœ… Post-release issues bar chart data validated");
                }
            }
        }
    }

    println!("âœ… All chart data generation validated");

    // Additional integration test: Test multiple metrics updates
    println!("\nðŸ”„ Additional Test: Multiple metrics updates and historical data");

    // Add more historical data
    for i in 1..5 {
        let mut historical_metrics = dashboard_metrics.clone();
        historical_metrics.timestamp = Utc::now() - Duration::hours(i * 6);

        // Slightly vary the metrics for realism
        if let Some(ref mut rm) = historical_metrics.release_metrics {
            rm.overall_success_rate = (aggregated_metrics.overall_success_rate - 0.01 * i as f64).max(0.8);
            rm.average_user_adoption = (aggregated_metrics.average_user_adoption - 0.1 * i as f64).max(5.0);
        }

        dashboard_service.update_metrics(historical_metrics);
    }

    // Test time range filtering
    let last_24h_range = TimeRange::Last24Hours;
    let metrics_24h = dashboard_service.get_metrics_for_range(&last_24h_range);
    assert!(!metrics_24h.is_empty(), "Should have metrics in last 24 hours");

    let last_7d_range = TimeRange::Last7Days;
    let metrics_7d = dashboard_service.get_metrics_for_range(&last_7d_range);
    assert!(metrics_7d.len() >= metrics_24h.len(), "Should have more metrics in 7 days than 24 hours");

    println!("âœ… Historical data integration tested");
    println!("   - 24h metrics: {}", metrics_24h.len());
    println!("   - 7d metrics: {}", metrics_7d.len());

    // Final validation: Generate comprehensive summary
    println!("\nðŸŽ¯ Final Validation: Comprehensive Integration Summary");

    let final_report = dashboard_service.generate_report(&release_view)?;
    let recommendations = &final_report.recommendations;

    println!("âœ… Final report generated with {} recommendations", recommendations.len());
    for (i, rec) in recommendations.iter().enumerate() {
        println!("   {}. {}", i + 1, rec);
    }

    // Validate that we have both duplicate prevention and release monitoring recommendations
    let has_release_recommendations = recommendations.iter().any(|r|
        r.contains("release") || r.contains("deployment") || r.contains("adoption")
    );
    let has_duplicate_recommendations = recommendations.iter().any(|r|
        r.contains("duplicate") || r.contains("prevention") || r.contains("accuracy")
    );

    assert!(has_release_recommendations, "Should have release-related recommendations");
    assert!(has_duplicate_recommendations, "Should have duplicate-related recommendations");

    println!("âœ… Integration test completed successfully!");
    println!("ðŸŽ‰ All components working together seamlessly:");
    println!("   - ReleaseMonitoringService âœ“");
    println!("   - DashboardService âœ“");
    println!("   - Metrics Integration âœ“");
    println!("   - Report Generation âœ“");
    println!("   - Alert System âœ“");
    println!("   - Chart Data Generation âœ“");
    println!("   - Historical Data Management âœ“");

    Ok(())
}

/// Generate alerts based on release metrics
fn generate_release_alerts(metrics: &AggregatedReleaseMetrics) -> Vec<ReleaseAlert> {
    let mut alerts = Vec::new();

    // Check success rate
    if metrics.overall_success_rate < 0.95 {
        alerts.push(ReleaseAlert {
            alert_type: ReleaseAlertType::LowSuccessRate,
            message: format!("Release success rate is below target: {:.1}% (target: 95%)",
                metrics.overall_success_rate * 100.0),
            severity: AlertSeverity::High,
            timestamp: Utc::now(),
        });
    }

    // Check post-release issues
    if metrics.average_post_release_issues > 5.0 {
        alerts.push(ReleaseAlert {
            alert_type: ReleaseAlertType::HighPostReleaseIssues,
            message: format!("High average post-release issues: {:.1} (threshold: 5.0)",
                metrics.average_post_release_issues),
            severity: AlertSeverity::Medium,
            timestamp: Utc::now(),
        });
    }

    // Check deployment time
    if metrics.average_deployment_time > 60.0 {
        alerts.push(ReleaseAlert {
            alert_type: ReleaseAlertType::DeploymentTimeExceeded,
            message: format!("Average deployment time exceeded: {:.1} min (threshold: 60 min)",
                metrics.average_deployment_time),
            severity: AlertSeverity::Medium,
            timestamp: Utc::now(),
        });
    }

    // Check user adoption
    if metrics.average_user_adoption < 5.0 {
        alerts.push(ReleaseAlert {
            alert_type: ReleaseAlertType::LowUserAdoption,
            message: format!("Low user adoption score: {:.1} (threshold: 5.0)",
                metrics.average_user_adoption),
            severity: AlertSeverity::Low,
            timestamp: Utc::now(),
        });
    }

    // Always add at least one informational alert for testing
    if alerts.is_empty() {
        alerts.push(ReleaseAlert {
            alert_type: ReleaseAlertType::LowSuccessRate,
            message: "All release metrics are within acceptable ranges".to_string(),
            severity: AlertSeverity::Low,
            timestamp: Utc::now(),
        });
    }

    alerts
}

#[tokio::test]
async fn test_release_monitoring_edge_cases() -> Result<()> {
    println!("ðŸ§ª Testing Release Monitoring Edge Cases");

    // Test with empty release data
    let rm_config = ReleaseMonitoringConfig {
        repository: "test-org/empty-repo".to_string(),
        metrics_storage_path: "empty_metrics.json".to_string(),
        ..Default::default()
    };

    let rm_service = ReleaseMonitoringService::new(rm_config);
    let empty_releases = Vec::new();

    let empty_metrics = rm_service.calculate_metrics(&empty_releases).await?;
    assert_eq!(empty_metrics.releases.len(), 0);
    assert_eq!(empty_metrics.overall_success_rate, 0.0);

    println!("âœ… Empty release data handled correctly");

    // Test with single release
    let single_release = vec![ReleaseData {
        tag_name: "v1.0.0".to_string(),
        name: "Single Release".to_string(),
        created_at: Utc::now() - Duration::days(1),
        published_at: Some(Utc::now() - Duration::hours(12)),
        draft: false,
        prerelease: false,
        body: "Single test release".to_string(),
        download_count: 100,
    }];

    let single_metrics = rm_service.calculate_metrics(&single_release).await?;
    assert_eq!(single_metrics.releases.len(), 1);
    assert_eq!(single_metrics.overall_success_rate, 1.0);

    println!("âœ… Single release data handled correctly");

    // Test dashboard with no release monitoring
    let dashboard_config_no_rm = DashboardConfig {
        release_monitoring_config: None,
        ..Default::default()
    };

    let dashboard_no_rm = DashboardService::new(dashboard_config_no_rm);
    let mut metrics_no_rm = DashboardMetrics {
        timestamp: Utc::now(),
        duplicate_stats: DuplicateStats::default(),
        prevention_stats: PreventionStats::default(),
        system_health: SystemHealth::default(),
        performance_metrics: PerformanceMetrics::default(),
        release_metrics: None,
    };

    dashboard_no_rm.update_metrics(metrics_no_rm);

    println!("âœ… Dashboard without release monitoring handled correctly");

    Ok(())
}
